{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Name: Sleepy Head Alert\n",
    "Developed By: Nishant Sharma\n",
    "### Project Summary:\n",
    "\n",
    "Objective:\n",
    "\n",
    "The project aims to develop a machine learning model to detect individuals sleeping in a car from uploaded videos or images. The model will:\n",
    "\n",
    "* Identify if a person is sleeping specifically in a car, not in other types of vehicles.\n",
    "* Count the number of individuals sleeping in the car.\n",
    "* Estimate the age of each sleeping person in the car.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "1. Vehicle Type Specificity:\n",
    "\n",
    "* The model will be trained to distinguish between different vehicle types and will only trigger detection if the sleeping person is in a car.\n",
    "\n",
    "2. Multiple Person Detection:\n",
    "* The model will be capable of detecting multiple people sleeping in the car, providing a count of the sleeping individuals.\n",
    "\n",
    "3. Age Estimation:\n",
    "* The model will predict the age of each detected sleeping person, giving an estimate of their age group.\n",
    "\n",
    "Project Scope:\n",
    "\n",
    "1. Data Collection and Annotation:\n",
    "\n",
    "* Extensive dataset collection of images and videos containing individuals in various states (sleeping, non-sleeping) and vehicle types, with detailed annotations.\n",
    "\n",
    "2. Model Development and Training:\n",
    "\n",
    "* Selection and training of an appropriate deep learning model using annotated datasets.\n",
    "* Application of transfer learning techniques to enhance model performance.\n",
    "\n",
    "3. Validation and Testing:\n",
    "\n",
    "* Rigorous validation using separate datasets to ensure model accuracy.\n",
    "* Real-world testing to evaluate model performance in diverse scenarios.\n",
    "\n",
    "4. Implementation and Integration:\n",
    "\n",
    "* Development of an interface for image/video upload and result display.\n",
    "* Implementation of a notification system for detected sleeping individuals.\n",
    "\n",
    "5. Deployment:\n",
    "\n",
    "* Deployment as a web or mobile application, ensuring accessibility and usability for end-users.\n",
    "\n",
    "Potential Applications:\n",
    "\n",
    "1. Driver Safety Monitoring:\n",
    "\n",
    "* Fleet Management: Monitoring drivers in commercial fleets to ensure they are not sleeping while parked, enhancing safety and operational efficiency.\n",
    "* Ridesharing Services: Ensuring passenger safety by detecting sleeping drivers in ridesharing vehicles during idle times.\n",
    "\n",
    "2. Surveillance and Security:\n",
    "\n",
    "* Parking Lots: Monitoring parking lots to detect and respond to individuals sleeping in cars, potentially identifying unauthorized or suspicious activity.\n",
    "* Residential Areas: Enhancing neighborhood security by detecting sleeping individuals in parked cars.\n",
    "\n",
    "3. Emergency Response:\n",
    "\n",
    "* Healthcare and Social Services: Identifying homeless individuals or those in distress who may be sleeping in cars, enabling timely intervention and support.\n",
    "\n",
    "4. Smart Cities and IoT:\n",
    "\n",
    "* Integrated City Surveillance: Incorporating the model into smart city initiatives for enhanced surveillance and public safety measures.\n",
    "* Connected Vehicles: Integrating the model with connected vehicle systems to provide real-time monitoring and alerts.\n",
    "\n",
    "5. Insurance and Claims:\n",
    "\n",
    "* Accident Prevention: Helping insurance companies assess the risk of drivers sleeping in cars and potentially preventing accidents by ensuring drivers are awake.\n",
    "* Claims Investigation: Assisting in the investigation of insurance claims related to accidents involving parked cars.\n",
    "\n",
    "6. Automotive Industry:\n",
    "\n",
    "* In-Vehicle Monitoring Systems: Integrating the model into advanced driver-assistance systems (ADAS) to monitor drivers' states and enhance safety features.\n",
    "\n",
    "By delivering accurate detection, counting, and age estimation of sleeping individuals in cars, this project can significantly contribute to various domains, improving safety, security, and operational efficiency.\n",
    "\n",
    "Github Link:\n",
    "\n",
    "### Problem Statement: Automated Detection and Analysis of Sleeping Individuals in Cars\n",
    "\n",
    "In the realm of vehicle safety and surveillance, there is a critical need for systems that can accurately detect and analyze individuals sleeping in cars. This need is driven by various factors, including ensuring driver and passenger safety, enhancing security measures in public and private spaces, and providing timely interventions in emergency situations. Existing solutions lack specificity and often fail to differentiate between types of vehicles or accurately count and analyze multiple individuals. Moreover, there is a gap in the ability to estimate the age of sleeping individuals, which can provide valuable demographic insights for various applications.\n",
    "\n",
    "### Let's Begin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 1: Data Preprocessing\n",
    "First, we'll load and preprocess the data from both datasets. Here's how we can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to the datasets\n",
    "car_data_path = \"C:\\\\Users\\\\nisha\\\\OneDrive\\\\Documents\\\\Car\"\n",
    "utkface_data_path = \"C:\\\\Users\\\\nisha\\\\OneDrive\\\\Documents\\\\Ethnicity Dataset\\\\UTKFace\"\n",
    "\n",
    "# Function to load images and labels from the car dataset\n",
    "def load_car_data(path):\n",
    "    categories = [\"Closed\", \"no_yawn\", \"Open\", \"yawn\"]\n",
    "    images = []\n",
    "    labels = []\n",
    "    for category in categories:\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(os.path.join(path, category)):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, category, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_img = cv2.resize(img_array, (128, 128))\n",
    "                images.append(resized_img)\n",
    "                labels.append(class_num)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(images), to_categorical(labels)\n",
    "\n",
    "# Load car data\n",
    "car_images, car_labels = load_car_data(car_data_path)\n",
    "\n",
    "# Function to load UTKFace data and extract age labels\n",
    "def load_utkface_data(path):\n",
    "    all_images = []\n",
    "    ages = []\n",
    "    for img_name in os.listdir(path):\n",
    "        parts = img_name.split('_')\n",
    "        if len(parts) >= 4:\n",
    "            age = int(parts[0])\n",
    "            img_array = cv2.imread(os.path.join(path, img_name))\n",
    "            resized_img = cv2.resize(img_array, (128, 128))\n",
    "            all_images.append(resized_img)\n",
    "            ages.append(age)\n",
    "    return np.array(all_images), np.array(ages)\n",
    "\n",
    "# Load UTKFace data\n",
    "utk_images, utk_ages = load_utkface_data(utkface_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 2: Model Development\n",
    "For this step, we'll develop a model to detect closed eyes, which will help us identify sleeping individuals in the car. We'll use a convolutional neural network (CNN), which is effective for image classification tasks. After we establish the eye detection model, we'll use the age prediction model trained on the UTKFace dataset to estimate ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 82ms/step - accuracy: 0.5159 - loss: 0.9793 - val_accuracy: 0.7672 - val_loss: 0.4254\n",
      "Epoch 2/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.7678 - loss: 0.4459 - val_accuracy: 0.8155 - val_loss: 0.3709\n",
      "Epoch 3/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.7883 - loss: 0.3826 - val_accuracy: 0.8569 - val_loss: 0.3151\n",
      "Epoch 4/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.8405 - loss: 0.3259 - val_accuracy: 0.8448 - val_loss: 0.3262\n",
      "Epoch 5/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.8510 - loss: 0.3075 - val_accuracy: 0.8483 - val_loss: 0.2787\n",
      "Epoch 6/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.8610 - loss: 0.2724 - val_accuracy: 0.8810 - val_loss: 0.2391\n",
      "Epoch 7/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.8816 - loss: 0.2590 - val_accuracy: 0.9052 - val_loss: 0.2215\n",
      "Epoch 8/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - accuracy: 0.9091 - loss: 0.2022 - val_accuracy: 0.9172 - val_loss: 0.1867\n",
      "Epoch 9/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.9392 - loss: 0.1686 - val_accuracy: 0.9207 - val_loss: 0.1649\n",
      "Epoch 10/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - accuracy: 0.9421 - loss: 0.1507 - val_accuracy: 0.9500 - val_loss: 0.1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Model architecture\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')  # Four classes: Closed, no_yawn, Open, yawn\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "eye_model = create_model()\n",
    "eye_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape car images for the model and split the data\n",
    "X_car = car_images.reshape(-1, 128, 128, 1) / 255.0  # Normalize the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_car, car_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = eye_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "eye_model.save(\"eye_detection_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 3: Integrating Age Prediction\n",
    "Now, let's integrate the age prediction using the pre-trained model from the UTKFace dataset. Since we don't have a pre-trained model yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 88ms/step - loss: 233.7865 - mae: 11.3228 - val_loss: 152.1389 - val_mae: 9.4745\n",
      "Epoch 2/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 81ms/step - loss: 136.6587 - mae: 8.7419 - val_loss: 141.0369 - val_mae: 8.8267\n",
      "Epoch 3/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 83ms/step - loss: 129.2094 - mae: 8.4782 - val_loss: 159.9019 - val_mae: 9.6936\n",
      "Epoch 4/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 82ms/step - loss: 114.6081 - mae: 7.9602 - val_loss: 138.2375 - val_mae: 8.8778\n",
      "Epoch 5/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 83ms/step - loss: 107.4817 - mae: 7.7270 - val_loss: 138.3454 - val_mae: 8.8644\n",
      "Epoch 6/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 83ms/step - loss: 108.2916 - mae: 7.7900 - val_loss: 136.5175 - val_mae: 8.4262\n",
      "Epoch 7/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 81ms/step - loss: 95.4920 - mae: 7.3316 - val_loss: 127.6645 - val_mae: 8.3686\n",
      "Epoch 8/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 81ms/step - loss: 89.0360 - mae: 7.1381 - val_loss: 131.6590 - val_mae: 8.3264\n",
      "Epoch 9/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 81ms/step - loss: 83.4695 - mae: 6.9047 - val_loss: 125.7229 - val_mae: 8.2829\n",
      "Epoch 10/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 84ms/step - loss: 78.0409 - mae: 6.6789 - val_loss: 131.0479 - val_mae: 8.3688\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load a pre-trained MobileNetV2 model, excluding the top layer\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Add custom layers for age prediction\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='linear')(x)  # Predicting age as a regression problem\n",
    "\n",
    "# This is the model we will train\n",
    "age_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the age model\n",
    "age_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Prepare the UTKFace images and ages for training\n",
    "X_utk = utk_images.astype('float32') / 255.0  # Normalize the data\n",
    "X_utk_train, X_utk_test, y_utk_train, y_utk_test = train_test_split(X_utk, utk_ages, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the age model\n",
    "age_history = age_model.fit(X_utk_train, y_utk_train, epochs=10, validation_data=(X_utk_test, y_utk_test))\n",
    "\n",
    "# Save the age model\n",
    "age_model.save(\"age_prediction_model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 4: Integration and Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000028BA18331A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000028BA18331A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 832ms/step\n",
      "Sleeping detected. Predicted age: 20.8\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the models\n",
    "eye_model = load_model(\"eye_detection_model.h5\")\n",
    "age_model = load_model(\"age_prediction_model.keras\")\n",
    "\n",
    "# Function to process an image and predict sleep and age\n",
    "def predict_sleep_and_age(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        return \"Image not found. Please check the file path.\"\n",
    "\n",
    "    try:\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img_gray = cv2.resize(img_gray, (128, 128))\n",
    "        img_gray = np.reshape(img_gray, (1, 128, 128, 1)) / 255.0\n",
    "        eye_pred = eye_model.predict(img_gray)\n",
    "        \n",
    "        sleep_status = np.argmax(eye_pred[0]) == 0  # Assuming 'Closed' class is index 0\n",
    "    \n",
    "        if sleep_status:\n",
    "            img_color = cv2.resize(img, (128, 128))\n",
    "            img_color = np.reshape(img_color, (1, 128, 128, 3)) / 255.0\n",
    "            age_pred = age_model.predict(img_color)\n",
    "            age = age_pred[0][0]\n",
    "            return f\"Sleeping detected. Predicted age: {age:.1f}\"\n",
    "        else:\n",
    "            return \"No sleeping detected.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "image_path = \"C:\\\\Users\\\\nisha\\\\OneDrive\\\\Documents\\\\Car\\\\Closed\\\\_24.jpg\"\n",
    "result = predict_sleep_and_age(image_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for the Tkinter GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load models\n",
    "eye_model = load_model(\"eye_detection_model.h5\")\n",
    "age_model = load_model(\"age_prediction_model.keras\")\n",
    "\n",
    "# Function to predict sleep and age\n",
    "def predict_sleep_and_age(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        return \"Image not found. Please check the file path.\"\n",
    "    try:\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img_gray = cv2.resize(img_gray, (128, 128))\n",
    "        img_gray = np.reshape(img_gray, (1, 128, 128, 1)) / 255.0\n",
    "        eye_pred = eye_model.predict(img_gray)\n",
    "        sleep_status = np.argmax(eye_pred[0]) == 0  # Assuming 'Closed' class is index 0\n",
    "        if sleep_status:\n",
    "            img_color = cv2.resize(img, (128, 128))\n",
    "            img_color = np.reshape(img_color, (1, 128, 128, 3)) / 255.0\n",
    "            age_pred = age_model.predict(img_color)\n",
    "            age = age_pred[0][0]\n",
    "            return f\"Sleeping detected. Predicted age: {age:.1f}\"\n",
    "        else:\n",
    "            return \"No sleeping detected.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Sleep Detection in Cars\")\n",
    "\n",
    "# Function to open an image file and automatically run detection\n",
    "def load_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((256, 256), Image.Resampling.LANCZOS)  # Use LANCZOS for high-quality downsampling\n",
    "        photo = ImageTk.PhotoImage(img)\n",
    "        img_label.image = photo\n",
    "        img_label.configure(image=photo)\n",
    "        img_label.file_path = file_path\n",
    "        result = predict_sleep_and_age(file_path)  # Automatically run detection\n",
    "        result_label.config(text=result)\n",
    "\n",
    "# Layout components\n",
    "load_button = tk.Button(window, text=\"Load Image\", command=load_image)\n",
    "load_button.pack(pady=10)\n",
    "\n",
    "img_label = tk.Label(window)\n",
    "img_label.pack(pady=10)\n",
    "\n",
    "result_label = tk.Label(window, text=\"Results will be shown here.\")\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "# Start the GUI\n",
    "window.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
